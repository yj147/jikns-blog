import { PrismaClient } from "@/lib/generated/prisma"

const prisma = new PrismaClient()

async function testNextSearch() {
  console.log("=== æµ‹è¯• Next.js æ–‡ç« æœç´¢ ===\n")

  // 1. æŸ¥çœ‹æ–‡ç« çš„ tokens
  const post = await prisma.$queryRaw<
    Array<{
      id: string
      title: string
      titleTokens: string | null
      contentTokens: string | null
    }>
  >`
    SELECT id, title, "titleTokens", LEFT("contentTokens", 100) as "contentTokens"
    FROM posts
    WHERE title LIKE '%Next.js%'
    LIMIT 1
  `
  console.log("ğŸ“„ æ–‡ç« ä¿¡æ¯:")
  console.table(post)

  // 2. æµ‹è¯•ä¸åŒæœç´¢è¯
  const queries = ["next", "next.js", "Next.js", "nextjs", "å…¨æ ˆ"]

  for (const q of queries) {
    const result = await prisma.$queryRaw<
      Array<{
        title: string
        matches: boolean
      }>
    >`
      SELECT title, search_vector @@ plainto_tsquery('simple', ${q}) as matches
      FROM posts
      WHERE title LIKE '%Next.js%'
      LIMIT 1
    `
    console.log(`\nğŸ” æœç´¢è¯: "${q}"`)
    console.log(`   åŒ¹é…: ${result[0]?.matches ? "âœ… YES" : "âŒ NO"}`)
  }

  // 3. æ£€æŸ¥ tokenizer å¦‚ä½•å¤„ç† "Next.js"
  console.log("\nğŸ“ åˆ†è¯æµ‹è¯•:")
  const { tokenizeText } = await import("@/lib/search/tokenizer")
  const samples = ["Next.js", "Next.js å…¨æ ˆå¼€å‘", "next", "next.js"]
  for (const text of samples) {
    const tokens = tokenizeText(text)
    console.log(`   "${text}" -> "${tokens}"`)
  }

  await prisma.$disconnect()
}

testNextSearch().catch(console.error)
